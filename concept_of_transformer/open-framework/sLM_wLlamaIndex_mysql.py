import os
import json
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime
from functools import lru_cache
from typing import List, Tuple, Dict, Optional
from sqlalchemy import create_engine, Column, Integer, String, Text, BLOB, text, DateTime
from sqlalchemy.orm import declarative_base, sessionmaker
from llama_index.core import SimpleDirectoryReader, VectorStoreIndex, Settings, Document
from llama_index.embeddings.huggingface import HuggingFaceEmbedding
from transformers import AutoTokenizer, AutoModelForCausalLM
from sklearn.metrics.pairwise import cosine_similarity

# MySQL ì—°ê²° ì„¤ì • (root ê³„ì • ì‚¬ìš©)
DATABASE_URL = "mysql+mysqlconnector://root:0000@localhost/rag_db"
engine = create_engine(DATABASE_URL)

# ë°ì´í„°ë² ì´ìŠ¤ê°€ ì—†ëŠ” ê²½ìš° ìƒì„±
def create_database():
    temp_engine = create_engine("mysql+mysqlconnector://root:0000@localhost/")
    with temp_engine.connect() as conn:
        conn.execute(text("CREATE DATABASE IF NOT EXISTS rag_db"))
        conn.commit()

create_database()

Base = declarative_base()

# MySQL í…Œì´ë¸” ì •ì˜
class DocumentModel(Base):
    __tablename__ = "documents"
    id = Column(Integer, primary_key=True)
    text = Column(Text)
    embedding = Column(BLOB)  # ë²¡í„° ì„ë² ë”© ì €ì¥
    doc_metadata = Column(Text)   # ë©”íƒ€ë°ì´í„° JSON ì €ì¥
    created_at = Column(DateTime, default=datetime.utcnow)  # ìƒì„± ì‹œê°„ ì¶”ê°€

Base.metadata.create_all(engine)
Session = sessionmaker(bind=engine)

# ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”
Settings.embed_model = HuggingFaceEmbedding(
    model_name="sentence-transformers/all-mpnet-base-v2"
)

class MySQLVectorStore:
    def __init__(self):
        self.embed_model = Settings.embed_model
        self.batch_size = 32
    
    @lru_cache(maxsize=1000)
    def _get_embedding(self, text: str) -> np.ndarray:
        """í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„± (ìºì‹± ì ìš©)"""
        embedding = np.array(self.embed_model.get_text_embedding(text))
        return embedding.astype('float32')
    
    def _batch_process(self, items: List, batch_size: int):
        """ë°°ì¹˜ ì²˜ë¦¬ ìœ í‹¸ë¦¬í‹°"""
        for i in range(0, len(items), batch_size):
            yield items[i:i + batch_size]
    
    def add_documents(self, documents: List[Document]):
        session = Session()
        try:
            # ê¸°ì¡´ ë¬¸ì„œ ì‚­ì œ
            session.query(DocumentModel).delete()
            session.commit()
            
            # ë°°ì¹˜ ì²˜ë¦¬ë¡œ ë¬¸ì„œ ì¶”ê°€
            for batch in self._batch_process(documents, self.batch_size):
                docs_to_add = []
                
                for doc in batch:
                    embedding = self._get_embedding(doc.text)
                    embedding_blob = json.dumps(embedding.tolist()).encode('utf-8')
                    
                    new_doc = DocumentModel(
                        text=doc.text,
                        embedding=embedding_blob,
                        doc_metadata=json.dumps(doc.metadata if hasattr(doc, 'metadata') else {})
                    )
                    docs_to_add.append(new_doc)
                
                # ë°ì´í„°ë² ì´ìŠ¤ì— ë°°ì¹˜ ì €ì¥
                session.bulk_save_objects(docs_to_add)
                session.commit()
            
            print(f"âœ… {len(documents)} ê°œì˜ ë¬¸ì„œê°€ ì„±ê³µì ìœ¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        finally:
            session.close()
    
    def query(self, query_text: str, top_k: int = 3) -> List[Tuple[float, DocumentModel]]:
        session = Session()
        try:
            # ì¿¼ë¦¬ ì„ë² ë”© ìƒì„±
            query_embedding = self._get_embedding(query_text).reshape(1, -1)
            
            # ëª¨ë“  ë¬¸ì„œ ê°€ì ¸ì˜¤ê¸°
            all_docs = session.query(DocumentModel).all()
            if not all_docs:
                return []
            
            # ë¬¸ì„œ ì„ë² ë”© í–‰ë ¬ ìƒì„±
            doc_embeddings = []
            for doc in all_docs:
                embedding = np.array(json.loads(doc.embedding.decode('utf-8')))
                doc_embeddings.append(embedding)
            doc_embeddings = np.array(doc_embeddings)
            
            # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
            similarities = cosine_similarity(query_embedding, doc_embeddings)[0]
            
            # ê²°ê³¼ ì •ë ¬ ë° ì¤‘ë³µ ì œê±°
            results = []
            seen_texts = set()
            
            # ìœ ì‚¬ë„ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬ëœ ì¸ë±ìŠ¤
            sorted_indices = np.argsort(similarities)[::-1]
            
            for idx in sorted_indices:
                doc = all_docs[idx]
                if doc.text not in seen_texts:
                    seen_texts.add(doc.text)
                    results.append((float(similarities[idx]), doc))
                    
                    if len(results) >= top_k:
                        break
            
            return results
        finally:
            session.close()

    def visualize_document_stats(self):
        """ë¬¸ì„œ í†µê³„ ì‹œê°í™”"""
        session = Session()
        try:
            # ë°ì´í„° ì¡°íšŒ
            docs = session.query(DocumentModel).all()
            
            # ê¸°ë³¸ ì •ë³´ ìˆ˜ì§‘
            doc_lengths = [len(doc.text) for doc in docs]
            created_times = [doc.created_at for doc in docs]
            
            # 1. ë¬¸ì„œ ê¸¸ì´ ë¶„í¬ ì‹œê°í™”
            plt.figure(figsize=(15, 5))
            
            plt.subplot(1, 2, 1)
            sns.histplot(doc_lengths)
            plt.title('ë¬¸ì„œ ê¸¸ì´ ë¶„í¬')
            plt.xlabel('ë¬¸ì„œ ê¸¸ì´ (ê¸€ì ìˆ˜)')
            plt.ylabel('ë¬¸ì„œ ìˆ˜')
            
            # 2. ì‹œê°„ë³„ ë¬¸ì„œ ì¶”ê°€ ì¶”ì´
            plt.subplot(1, 2, 2)
            sns.scatterplot(x=range(len(created_times)), y=created_times)
            plt.title('ë¬¸ì„œ ì¶”ê°€ ì‹œê°„ ì¶”ì´')
            plt.xlabel('ë¬¸ì„œ ì¸ë±ìŠ¤')
            plt.ylabel('ìƒì„± ì‹œê°„')
            
            plt.tight_layout()
            plt.savefig('document_stats.png')
            plt.close()
            
            # 3. ë°ì´í„°ë² ì´ìŠ¤ í†µê³„ ì¶œë ¥
            total_docs = len(docs)
            avg_length = np.mean(doc_lengths)
            
            print("\nğŸ“Š ë°ì´í„°ë² ì´ìŠ¤ í†µê³„:")
            print(f"- ì´ ë¬¸ì„œ ìˆ˜: {total_docs}")
            print(f"- í‰ê·  ë¬¸ì„œ ê¸¸ì´: {avg_length:.2f} ê¸€ì")
            print(f"- ê°€ì¥ ê¸´ ë¬¸ì„œ: {max(doc_lengths)} ê¸€ì")
            print(f"- ê°€ì¥ ì§§ì€ ë¬¸ì„œ: {min(doc_lengths)} ê¸€ì")
            
            # 4. ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ë³€í™˜í•˜ì—¬ í‘œ í˜•íƒœë¡œ ì¶œë ¥
            df = pd.DataFrame({
                'ID': [doc.id for doc in docs],
                'í…ìŠ¤íŠ¸ (ì¼ë¶€)': [doc.text[:50] + '...' for doc in docs],
                'ê¸¸ì´': doc_lengths,
                'ìƒì„± ì‹œê°„': created_times
            })
            
            print("\nğŸ“‹ ë¬¸ì„œ ëª©ë¡:")
            print(df.to_string(index=False))
            
        finally:
            session.close()

# ìƒ˜í”Œ ë°ì´í„° ìƒì„± ë° ì²˜ë¦¬
def create_sample_documents():
    try:
        # PDF íŒŒì¼ì—ì„œ ë¬¸ì„œ ë¡œë“œ
        documents = SimpleDirectoryReader(
            input_files=["data/resume.pdf"]
        ).load_data()
        
        if not documents:
            print("âš ï¸ PDF íŒŒì¼ì—ì„œ ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ìƒ˜í”Œ ë°ì´í„°ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
            return _create_fallback_documents()
            
        print(f"âœ… PDF íŒŒì¼ì—ì„œ {len(documents)}ê°œì˜ ë¬¸ì„œë¥¼ ì„±ê³µì ìœ¼ë¡œ ë¶ˆëŸ¬ì™”ìŠµë‹ˆë‹¤.")
        return documents
    except Exception as e:
        print(f"âš ï¸ PDF íŒŒì¼ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        print("ìƒ˜í”Œ ë°ì´í„°ë¥¼ ëŒ€ì‹  ì‚¬ìš©í•©ë‹ˆë‹¤.")
        return _create_fallback_documents()


def _create_fallback_documents():
    """PDF íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨ ì‹œ ì‚¬ìš©í•  ìƒ˜í”Œ ë°ì´í„°"""
    sample_texts = [
        "ê¸°ê³„ í•™ìŠµì€ ì¸ê³µì§€ëŠ¥ì˜ í•œ ë¶„ì•¼ë¡œ, ì»´í“¨í„°ê°€ ë°ì´í„°ë¡œë¶€í„° í•™ìŠµí•˜ì—¬ íŒ¨í„´ì„ ì°¾ì•„ë‚´ëŠ” ê¸°ìˆ ì…ë‹ˆë‹¤.",
        "ë”¥ëŸ¬ë‹ì€ ì—¬ëŸ¬ ì¸µì˜ ì¸ê³µ ì‹ ê²½ë§ì„ ì‚¬ìš©í•˜ì—¬ ë°ì´í„°ë¥¼ í•™ìŠµí•˜ëŠ” ê¸°ê³„ í•™ìŠµì˜ í•œ ë°©ë²•ì…ë‹ˆë‹¤.",
        "ìì—°ì–´ ì²˜ë¦¬ëŠ” ì»´í“¨í„°ê°€ ì¸ê°„ì˜ ì–¸ì–´ë¥¼ ì´í•´í•˜ê³  ì²˜ë¦¬í•˜ëŠ” ê¸°ìˆ ì„ ì—°êµ¬í•˜ëŠ” ë¶„ì•¼ì…ë‹ˆë‹¤.",
        "ê°•í™”í•™ìŠµì€ ì—ì´ì „íŠ¸ê°€ í™˜ê²½ê³¼ ìƒí˜¸ì‘ìš©í•˜ë©´ì„œ ë³´ìƒì„ ìµœëŒ€í™”í•˜ëŠ” ë°©í–¥ìœ¼ë¡œ í•™ìŠµí•˜ëŠ” ë°©ë²•ì…ë‹ˆë‹¤.",
        "ì „ì´í•™ìŠµì€ í•˜ë‚˜ì˜ ì‘ì—…ì—ì„œ í•™ìŠµí•œ ì§€ì‹ì„ ë‹¤ë¥¸ ì‘ì—…ì— ì ìš©í•˜ëŠ” ê¸°ê³„í•™ìŠµ ê¸°ë²•ì…ë‹ˆë‹¤."
    ]
    
    documents = [Document(text=text) for text in sample_texts]
    return documents

def process_documents():
    # ìƒ˜í”Œ ë°ì´í„° ìƒì„±
    documents = create_sample_documents()
    
    vector_store = MySQLVectorStore()
    vector_store.add_documents(documents)
    
    # ë°ì´í„°ë² ì´ìŠ¤ í†µê³„ ì‹œê°í™”
    vector_store.visualize_document_stats()

# RAG ì§ˆì˜ ì²˜ë¦¬
def rag_query(query_text: str) -> List[Tuple[float, DocumentModel]]:
    vector_store = MySQLVectorStore()
    results = vector_store.query(query_text)
    
    print("\nğŸ” ê²€ìƒ‰ ê²°ê³¼ ìƒìœ„ 3ê°œ ë¬¸ì„œ:")
    for i, (score, doc) in enumerate(results, 1):
        print(f"{i}. (ìœ ì‚¬ë„: {score:.4f}) {doc.text[:200]}...")
    
    return results

# í…ŒìŠ¤íŠ¸ ì‹¤í–‰
if __name__ == "__main__":
    # ìƒ˜í”Œ ë°ì´í„° ì´ˆê¸°í™”
    process_documents()
    
    # í…ŒìŠ¤íŠ¸ ì§ˆì˜ë“¤
    test_queries = [
        "ê¸°ê³„ í•™ìŠµì˜ ì£¼ìš” ê°œë…ì€ ë¬´ì—‡ì¸ê°€ìš”?",
        "ë”¥ëŸ¬ë‹ì´ë€ ë¬´ì—‡ì¸ê°€ìš”?",
        "ìì—°ì–´ ì²˜ë¦¬ëŠ” ì–´ë–¤ ë¶„ì•¼ì¸ê°€ìš”?",
        "ê°•í™”í•™ìŠµì— ëŒ€í•´ ì„¤ëª…í•´ì£¼ì„¸ìš”",
        "ì „ì´í•™ìŠµì´ë€ ë¬´ì—‡ì¸ê°€ìš”?"
    ]
    
    for query in test_queries:
        print(f"\nğŸ“ ì§ˆì˜: {query}")
        rag_query(query) 