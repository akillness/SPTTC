{
    "cells": [
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "# DeepSeek-R1 파인튜닝 모델 추론 (Inference)\n",
       "\n",
       "이 노트북은 파인튜닝된 DeepSeek-R1 모델을 사용하여 페르소나 기반 응답을 생성하는 코드를 제공합니다."
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## 필요한 라이브러리 설치\n",
       "\n",
       "먼저 필요한 라이브러리들을 설치합니다:"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
       "!pip install torch transformers peft"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## 라이브러리 임포트"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
       "import torch\n",
       "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
       "from peft import PeftModel, PeftConfig"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## 모델 로드 함수"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
       "def load_model(model_path):\n",
       "    \"\"\"\n",
       "    학습된 모델과 토크나이저를 로드합니다.\n",
       "    \"\"\"\n",
       "    print(f\"Loading model from {model_path}...\")\n",
       "    \n",
       "    # 토크나이저 로드\n",
       "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
       "    \n",
       "    # LoRA 어댑터가 저장된 모델인 경우\n",
       "    try:\n",
       "        # PeftModel 설정 로드\n",
       "        config = PeftConfig.from_pretrained(model_path)\n",
       "        # 기본 모델 로드\n",
       "        base_model = AutoModelForCausalLM.from_pretrained(\n",
       "            config.base_model_name_or_path, \n",
       "            torch_dtype=torch.bfloat16,\n",
       "            device_map=\"auto\",\n",
       "            trust_remote_code=True\n",
       "        )\n",
       "        # LoRA 어댑터 적용\n",
       "        model = PeftModel.from_pretrained(base_model, model_path)\n",
       "    except:\n",
       "        # 전체 모델이 저장된 경우\n",
       "        model = AutoModelForCausalLM.from_pretrained(\n",
       "            model_path,\n",
       "            torch_dtype=torch.bfloat16,\n",
       "            device_map=\"auto\",\n",
       "            trust_remote_code=True\n",
       "        )\n",
       "    \n",
       "    return model, tokenizer"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## 응답 생성 함수"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
       "def generate_response(model, tokenizer, persona, question, \n",
       "                     max_length=512, temperature=0.7, top_p=0.95):\n",
       "    \"\"\"\n",
       "    입력된 페르소나와 질문에 대한 응답을 생성합니다.\n",
       "    \"\"\"\n",
       "    # 입력 형식 구성\n",
       "    input_text = f\"<persona>\\n{persona}\\n</persona>\\n\\n<human>\\n{question}\\n</human>\\n\\n<assistant>\\n\"\n",
       "    \n",
       "    # 토큰화\n",
       "    inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
       "    \n",
       "    # 응답 생성\n",
       "    with torch.no_grad():\n",
       "        outputs = model.generate(\n",
       "            inputs[\"input_ids\"],\n",
       "            max_length=max_length,\n",
       "            temperature=temperature,\n",
       "            do_sample=True,\n",
       "            top_p=top_p,\n",
       "            pad_token_id=tokenizer.pad_token_id or tokenizer.eos_token_id\n",
       "        )\n",
       "    \n",
       "    # 응답 디코딩\n",
       "    full_response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
       "    \n",
       "    # 응답 부분만 추출\n",
       "    assistant_part = full_response.split(\"<assistant>\")[-1].strip()\n",
       "    \n",
       "    return assistant_part"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## 모델 로드\n",
       "\n",
       "파인튜닝된 모델을 로드합니다. 모델 경로를 필요에 따라 수정하세요."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
       "model_path = \"./deepseek-r1-finetuned\"\n",
       "\n",
       "try:\n",
       "    model, tokenizer = load_model(model_path)\n",
       "    print(\"모델 로드 성공!\")\n",
       "except Exception as e:\n",
       "    print(f\"모델 로드 중 오류 발생: {e}\")\n",
       "    print(\"대신 원본 모델을 로드합니다.\")\n",
       "    model, tokenizer = load_model(\"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\")"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## 페르소나 설정\n",
       "\n",
       "모델이 응답할 때 사용할 페르소나를 정의합니다."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
       "# 기본 페르소나 설정\n",
       "default_persona = \"20대 남성, 대학생, 취미는 게임과 독서, 성격은 친절하고 차분함\"\n",
       "\n",
       "# 사용자가 원하는 페르소나 입력 가능\n",
       "custom_persona = input(\"페르소나를 입력하세요 (비워두면 기본값 사용): \").strip()\n",
       "\n",
       "persona = custom_persona if custom_persona else default_persona\n",
       "print(f\"사용할 페르소나: {persona}\")"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## 단일 대화 테스트\n",
       "\n",
       "페르소나와 질문을 통해 응답을 생성해봅니다."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
       "question = \"안녕하세요, 자기소개 좀 해주세요.\"\n",
       "\n",
       "response = generate_response(model, tokenizer, persona, question, \n",
       "                            temperature=0.7, top_p=0.95)\n",
       "\n",
       "print(f\"질문: {question}\")\n",
       "print(f\"응답: {response}\")"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## 대화형 채팅 인터페이스\n",
       "\n",
       "사용자와 대화형으로 채팅할 수 있는 인터페이스를 제공합니다."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
       "print(\"\\n====== DeepSeek-R1 페르소나 채팅 ======\\n\")\n",
       "print(\"대화를 시작합니다. 종료하려면 'quit' 또는 'exit'를 입력하세요.\\n\")\n",
       "\n",
       "while True:\n",
       "    # 질문 입력\n",
       "    question = input(\"사용자: \").strip()\n",
       "    if question.lower() in [\"quit\", \"exit\"]:\n",
       "        print(\"채팅을 종료합니다.\")\n",
       "        break\n",
       "    \n",
       "    # 응답 생성\n",
       "    try:\n",
       "        response = generate_response(model, tokenizer, persona, question)\n",
       "        print(f\"어시스턴트: {response}\\n\")\n",
       "    except Exception as e:\n",
       "        print(f\"응답 생성 중 오류 발생: {e}\")"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## 생성 파라미터 조정 실험\n",
       "\n",
       "다양한 파라미터로 응답을 생성하여 결과를 비교해볼 수 있습니다."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
       "test_question = \"오늘 기분이 어때요?\"\n",
       "\n",
       "# 온도(temperature) 값 변경 실험\n",
       "temperatures = [0.3, 0.7, 1.0]\n",
       "\n",
       "print(\"온도(temperature) 변경에 따른 응답 비교:\")\n",
       "for temp in temperatures:\n",
       "    response = generate_response(model, tokenizer, persona, test_question, \n",
       "                              temperature=temp, top_p=0.95)\n",
       "    print(f\"\\n온도 {temp}일 때 응답:\\n{response}\")"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
       "# top_p 값 변경 실험\n",
       "top_p_values = [0.7, 0.9, 0.99]\n",
       "\n",
       "print(\"top_p 변경에 따른 응답 비교:\")\n",
       "for top_p_val in top_p_values:\n",
       "    response = generate_response(model, tokenizer, persona, test_question, \n",
       "                              temperature=0.7, top_p=top_p_val)\n",
       "    print(f\"\\ntop_p {top_p_val}일 때 응답:\\n{response}\")"
      ]
     }
    ],
    "metadata": {
     "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
     },
     "language_info": {
      "codemirror_mode": {
       "name": "ipython",
       "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
     }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}