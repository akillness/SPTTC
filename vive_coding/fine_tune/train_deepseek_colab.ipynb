{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wv7CDMJmw5Et"
   },
   "source": [
    "# DeepSeek-R1 파인튜닝 노트북\n",
    "\n",
    "이 노트북은 DeepSeek-R1-Distill-Qwen-1.5B 모델을 페르소나 대화 데이터셋으로 파인튜닝하기 위한 코드입니다.\n",
    "L4 GPU에 최적화되어 있으며, 4bit 양자화를 사용하지 않고 mixed precision(bf16)을 활용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 22096,
     "status": "ok",
     "timestamp": 1743855757083,
     "user": {
      "displayName": "정장영 (JEO)",
      "userId": "12533087895771299388"
     },
     "user_tz": -540
    },
    "id": "gpnrB0ZR0G4q",
    "outputId": "c88ac57e-a3cf-44ad-8b95-deb26fe72f16"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 77897,
     "status": "ok",
     "timestamp": 1743855732466,
     "user": {
      "displayName": "정장영 (JEO)",
      "userId": "12533087895771299388"
     },
     "user_tz": -540
    },
    "id": "3lDHd4wIw5Ew",
    "outputId": "86f92b0c-06c0-4203-c5c9-7dd1bec0000e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.50.3)\n",
      "Collecting datasets\n",
      "  Downloading datasets-3.5.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
      "Collecting fireducks\n",
      "  Downloading fireducks-1.2.5-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (1.0 kB)\n",
      "Requirement already satisfied: tensorboard in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
      "Collecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets)\n",
      "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec<=2024.12.0,>=2023.1.0 (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets)\n",
      "  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.0)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
      "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
      "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
      "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
      "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
      "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
      "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Collecting firefw==1.2.5 (from fireducks)\n",
      "  Downloading firefw-1.2.5-py3-none-any.whl.metadata (818 bytes)\n",
      "Collecting pyarrow>=15.0.0 (from datasets)\n",
      "  Downloading pyarrow-19.0.1-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
      "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (1.4.0)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (1.71.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (3.7)\n",
      "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (5.29.4)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (75.2.0)\n",
      "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (1.17.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (3.1.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.3.1)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard) (3.0.2)\n",
      "Downloading datasets-3.5.0-py3-none-any.whl (491 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.2/491.2 kB\u001b[0m \u001b[31m28.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m101.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m88.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m56.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m38.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m96.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading fireducks-1.2.5-cp311-cp311-manylinux_2_28_x86_64.whl (7.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.3/7.3 MB\u001b[0m \u001b[31m112.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading firefw-1.2.5-py3-none-any.whl (12 kB)\n",
      "Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyarrow-19.0.1-cp311-cp311-manylinux_2_28_x86_64.whl (42.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.1/42.1 MB\u001b[0m \u001b[31m53.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: xxhash, pyarrow, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, fsspec, firefw, dill, nvidia-cusparse-cu12, nvidia-cudnn-cu12, multiprocess, nvidia-cusolver-cu12, fireducks, datasets\n",
      "  Attempting uninstall: pyarrow\n",
      "    Found existing installation: pyarrow 18.1.0\n",
      "    Uninstalling pyarrow-18.1.0:\n",
      "      Successfully uninstalled pyarrow-18.1.0\n",
      "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
      "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
      "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-curand-cu12\n",
      "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
      "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
      "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
      "  Attempting uninstall: nvidia-cufft-cu12\n",
      "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
      "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
      "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
      "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
      "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
      "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
      "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cublas-cu12\n",
      "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
      "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
      "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2025.3.2\n",
      "    Uninstalling fsspec-2025.3.2:\n",
      "      Successfully uninstalled fsspec-2025.3.2\n",
      "  Attempting uninstall: nvidia-cusparse-cu12\n",
      "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
      "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
      "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
      "  Attempting uninstall: nvidia-cudnn-cu12\n",
      "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
      "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
      "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
      "  Attempting uninstall: nvidia-cusolver-cu12\n",
      "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
      "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
      "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2024.12.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed datasets-3.5.0 dill-0.3.8 fireducks-1.2.5 firefw-1.2.5 fsspec-2024.12.0 multiprocess-0.70.16 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 pyarrow-19.0.1 xxhash-3.5.0\n"
     ]
    }
   ],
   "source": [
    "# 필요한 라이브러리 설치\n",
    "!pip install transformers datasets torch fireducks tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2518,
     "status": "ok",
     "timestamp": 1743855734986,
     "user": {
      "displayName": "정장영 (JEO)",
      "userId": "12533087895771299388"
     },
     "user_tz": -540
    },
    "id": "ePLSDUPZw5Ex",
    "outputId": "e8dedac6-c10a-4e60-d159-664f0d1a3e7d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: peft in /usr/local/lib/python3.11/dist-packages (0.14.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from peft) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from peft) (24.2)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from peft) (5.9.5)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from peft) (6.0.2)\n",
      "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.11/dist-packages (from peft) (2.6.0+cu124)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (from peft) (4.50.3)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from peft) (1.5.2)\n",
      "Requirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from peft) (0.5.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.25.0 in /usr/local/lib/python3.11/dist-packages (from peft) (0.30.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.25.0->peft) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.25.0->peft) (2024.12.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.25.0->peft) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.25.0->peft) (4.13.0)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.13.0->peft) (1.3.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers->peft) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers->peft) (0.21.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.13.0->peft) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.25.0->peft) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.25.0->peft) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.25.0->peft) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.25.0->peft) (2025.1.31)\n"
     ]
    }
   ],
   "source": [
    "!pip install peft tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 17256,
     "status": "ok",
     "timestamp": 1743855774340,
     "user": {
      "displayName": "정장영 (JEO)",
      "userId": "12533087895771299388"
     },
     "user_tz": -540
    },
    "id": "TU3pBeASw5Ex"
   },
   "outputs": [],
   "source": [
    "# 필요한 라이브러리 임포트\n",
    "import os\n",
    "import glob\n",
    "# import pandas as pd\n",
    "import fireducks.pandas as pd\n",
    "import torch\n",
    "import unicodedata\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    TrainerCallback,\n",
    "    TrainerState,\n",
    "    TrainerControl\n",
    ")\n",
    "from datasets import Dataset\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from tqdm import tqdm\n",
    "from transformers.integrations import TensorBoardCallback\n",
    "from huggingface_hub import login\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qOUxUugMw5Ex"
   },
   "source": [
    "## 1. 설정 및 하이퍼파라미터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1743855774348,
     "user": {
      "displayName": "정장영 (JEO)",
      "userId": "12533087895771299388"
     },
     "user_tz": -540
    },
    "id": "xIcfsno6w5Ey"
   },
   "outputs": [],
   "source": [
    "# 데이터 경로 설정\n",
    "DATA_DIR = os.path.abspath(\"/content/drive/MyDrive/ToyProject/for_Colab/LiarHeart_dataset/\")\n",
    "MODEL_ID = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    "OUTPUT_DIR = os.path.join(DATA_DIR, \"deepseek-r1-finetuned\")\n",
    "TB_LOG_DIR = os.path.join(OUTPUT_DIR, \"tensorboard_logs\")\n",
    "\n",
    "# LoRA 설정 (메모리 효율성 중심)\n",
    "LORA_R = 8                # 기존 16 → 낮은 랭크로 메모리 절약\n",
    "LORA_ALPHA = 16           # alpha = 2*R 권장\n",
    "LORA_DROPOUT = 0.15       # 약간의 정규화 강화\n",
    "LORA_TARGET_MODULES = [\n",
    "    \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "    \"gate_proj\", \"up_proj\", \"down_proj\"  # MLP 레이어 추가\n",
    "]\n",
    "\n",
    "# 학습 설정 (L4 최적화)\n",
    "BATCH_SIZE = 4                     # 기존 2 → L4에서 가능한 최대 배치\n",
    "GRADIENT_ACCUMULATION_STEPS = 8    # 기존 16 → 유효 배치 크기 유지 (4×8=32)\n",
    "LEARNING_RATE = 3e-4               # 기존 5e-4 → 낮은 LR로 안정성 확보\n",
    "NUM_EPOCHS = 1                     # 변동 없음 (소형 모델 특성)\n",
    "MAX_LENGTH = 512                   # 변동 없음 (VRAM 한계)\n",
    "WARMUP_RATIO = 0.05                # 기존 0.1 → 빠른 워밍업\n",
    "WEIGHT_DECAY = 0.01                # 기존 0.05 → 과적합 방지 조정\n",
    "\n",
    "# 처리할 시트 목록\n",
    "SHEET_NAMES = [\"알리바이_대화\", \"인터뷰_대화\", \"가쉽_대화\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xwW3Br06w5Ey"
   },
   "source": [
    "## 2. 커스텀 콜백 클래스 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1743855774353,
     "user": {
      "displayName": "정장영 (JEO)",
      "userId": "12533087895771299388"
     },
     "user_tz": -540
    },
    "id": "6XZxDQPww5Ey"
   },
   "outputs": [],
   "source": [
    "# 커스텀 콜백 클래스\n",
    "class CustomCallback(TrainerCallback):\n",
    "    def on_step_end(self, args, state, control, **kwargs):\n",
    "        if state.global_step % 10 == 0:  # 10 스텝마다 진행상황 출력\n",
    "            print(f\"Step {state.global_step}/{state.max_steps} - Loss: {state.log_history[-1]['loss']:.4f}\")\n",
    "        return control\n",
    "\n",
    "    def on_epoch_end(self, args, state, control, **kwargs):\n",
    "        print(f\"\\nEpoch {state.epoch} completed\\n\")\n",
    "        return control\n",
    "\n",
    "    def on_evaluate(self, args, state, control, metrics=None, **kwargs):\n",
    "        if metrics:\n",
    "            print(f\"\\n===== Evaluation Results at Step {state.global_step} =====\")\n",
    "            for key, value in metrics.items():\n",
    "                print(f\"{key}: {value:.4f}\")\n",
    "            print(\"=\" * 50)\n",
    "        return control"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XeQvouaaw5Ez"
   },
   "source": [
    "## 3. 데이터 로딩 및 준비 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1743855774369,
     "user": {
      "displayName": "정장영 (JEO)",
      "userId": "12533087895771299388"
     },
     "user_tz": -540
    },
    "id": "IflBPwrOw5Ez"
   },
   "outputs": [],
   "source": [
    "def load_and_prepare_data():\n",
    "    # Excel 파일 찾기\n",
    "    search_prefix = \"페르소나 데이터_\"\n",
    "    search_suffix = \".xlsx\"\n",
    "    normalized_prefix = unicodedata.normalize('NFC', search_prefix)\n",
    "\n",
    "    print(f\"Searching for files in {DATA_DIR}\")\n",
    "    EXCEL_FILES = []\n",
    "    for filename in os.listdir(DATA_DIR):\n",
    "        normalized_filename = unicodedata.normalize('NFC', filename)\n",
    "        if normalized_filename.startswith(normalized_prefix) and normalized_filename.endswith(search_suffix) and not normalized_filename.startswith(\"~$\"):\n",
    "            EXCEL_FILES.append(os.path.join(DATA_DIR, filename))\n",
    "    print(f\"Found Excel files: {EXCEL_FILES}\")\n",
    "\n",
    "    dialogue_datas = []\n",
    "    for excel_file in tqdm(EXCEL_FILES, desc=\"Loading Excel files\"):\n",
    "        print(f\"Processing file: {excel_file}\")\n",
    "\n",
    "        persona_name = excel_file.split('_')[-1].split('.')[0]\n",
    "        person_dialogue_dfs = []\n",
    "        for sheet_name in SHEET_NAMES:\n",
    "            try:\n",
    "                print(f\"  Reading sheet: {sheet_name}\")\n",
    "                # Read Excel with string type for all columns\n",
    "                df = pd.read_excel(\n",
    "                    excel_file,\n",
    "                    sheet_name=sheet_name,\n",
    "                    dtype=str  # Force string type during reading\n",
    "                )\n",
    "                print(f\"  Columns in sheet {sheet_name}: {df.columns.tolist()}\")\n",
    "\n",
    "                # Select only the required columns\n",
    "                required_columns = ['사람 대사', '챗봇 대사', '감정']\n",
    "                if all(col in df.columns for col in required_columns):\n",
    "                    df = df[required_columns].copy()\n",
    "                    # Clean the data\n",
    "                    df = df.fillna('')  # Replace NaN with empty string\n",
    "                    df['이름'] = persona_name  # Add persona name\n",
    "                    person_dialogue_dfs.append(df)\n",
    "                else:\n",
    "                    print(f\"  Warning: Required columns not found in sheet {sheet_name}\")\n",
    "                    print(f\"  Available columns: {df.columns.tolist()}\")\n",
    "                    print(f\"  Required columns: {required_columns}\")\n",
    "            except Exception as e:\n",
    "                print(f\"  Error processing {excel_file}, sheet {sheet_name}: {e}\")\n",
    "\n",
    "        if person_dialogue_dfs:\n",
    "            dialoguse = pd.concat(person_dialogue_dfs, ignore_index=True)\n",
    "            dialogue_datas.append(dialoguse)\n",
    "\n",
    "    if not dialogue_datas:\n",
    "        raise ValueError(\"No data was loaded from the Excel files\")\n",
    "\n",
    "    persona_datas = pd.concat(dialogue_datas, ignore_index=True)\n",
    "\n",
    "    # Clean and prepare data\n",
    "    persona_datas = persona_datas.fillna('')  # Replace any remaining NaN\n",
    "    persona_datas = persona_datas.astype(str)  # Ensure string type\n",
    "\n",
    "    # Rename columns to match expected format\n",
    "    column_mapping = {\n",
    "        '사람 대사': 'Q',\n",
    "        '챗봇 대사': 'A',\n",
    "        '감정': 'E',\n",
    "        '이름': 'N'\n",
    "    }\n",
    "    persona_datas = persona_datas.rename(columns=column_mapping)\n",
    "\n",
    "    # Handle emotion field\n",
    "    persona_datas['E'] = persona_datas['E'].replace({'': '감정없음', 'nan': '감정없음', 'None': '감정없음'})\n",
    "\n",
    "    # Remove any rows with empty essential fields\n",
    "    persona_datas = persona_datas[\n",
    "        (persona_datas['Q'].str.strip() != '') &\n",
    "        (persona_datas['A'].str.strip() != '')\n",
    "    ].reset_index(drop=True)\n",
    "\n",
    "    print(f\"Total examples loaded: {len(persona_datas)}\")\n",
    "    print(f\"Final columns: {persona_datas.columns.tolist()}\")\n",
    "\n",
    "    # Format the text for training\n",
    "    texts = []\n",
    "    for _, row in persona_datas.iterrows():\n",
    "        formatted_text = f\"Human: {row['Q'].strip()}\\nAssistant: {row['A'].strip()}\\nEmotion: {row['E'].strip()}\\nName: {row['N'].strip()}\"\n",
    "        # formatted_text = f'<startoftext>이름:{row.N}\\n질문:{row.Q}\\n답변:{row.A}\\n감정: {row.E} <endoftext>'\n",
    "        texts.append(formatted_text)\n",
    "\n",
    "    # 학습:검증 데이터 분리 (9:1)\n",
    "    train_size = int(len(texts) * 0.9)\n",
    "\n",
    "    # Create datasets directly from lists\n",
    "    try:\n",
    "        train_dataset = Dataset.from_dict({\"text\": texts[:train_size]})\n",
    "        eval_dataset = Dataset.from_dict({\"text\": texts[train_size:]})\n",
    "\n",
    "        # Verify the datasets\n",
    "        print(\"Train dataset size:\", len(train_dataset))\n",
    "        print(\"Eval dataset size:\", len(eval_dataset))\n",
    "\n",
    "        # Verify data type\n",
    "        print(\"Sample text from train dataset:\", train_dataset[0]['text'])\n",
    "\n",
    "        return train_dataset, eval_dataset\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Error creating datasets:\", str(e))\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jjhWDiAYw5Ez"
   },
   "source": [
    "## 4. 모델 및 토크나이저 준비 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1743855774386,
     "user": {
      "displayName": "정장영 (JEO)",
      "userId": "12533087895771299388"
     },
     "user_tz": -540
    },
    "id": "Rd9FkDGMw5Ez"
   },
   "outputs": [],
   "source": [
    "def prepare_model_and_tokenizer():\n",
    "    print(\"Loading model and tokenizer...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_ID,\n",
    "                                              # bos_token='<startoftext>',\n",
    "                                              # eos_token='<endoftext>',\n",
    "                                              use_fast=False,\n",
    "                                              cache_dir=DATA_DIR)\n",
    "    # tokenizer.pad_token_id = (\n",
    "    #     0  # 엉. 우리는 이것이 EOS 토큰과 다르기를 원합니다.\n",
    "    # )\n",
    "    # Load in 8bit to save memory\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_ID,\n",
    "        torch_dtype=torch.float16,  # Use float16 for better memory efficiency\n",
    "        device_map=\"auto\",\n",
    "        cache_dir=DATA_DIR\n",
    "    )\n",
    "\n",
    "    # Enable gradient computation\n",
    "    model.train()  # Set to training mode\n",
    "    model.config.use_cache = False  # Disable cache for training\n",
    "\n",
    "    # LoRA 설정 적용\n",
    "    peft_config = LoraConfig(\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "        r=LORA_R,\n",
    "        lora_alpha=LORA_ALPHA,\n",
    "        lora_dropout=LORA_DROPOUT,\n",
    "        target_modules=LORA_TARGET_MODULES,\n",
    "        bias=\"none\",\n",
    "        inference_mode=False,\n",
    "    )\n",
    "\n",
    "    model = get_peft_model(model, peft_config)\n",
    "\n",
    "    # Enable training for all LoRA parameters\n",
    "    for name, param in model.named_parameters():\n",
    "        if \"lora\" in name:\n",
    "            param.requires_grad = True\n",
    "            print(f\"Parameter {name} requires_grad: {param.requires_grad}\")\n",
    "        else:\n",
    "            param.requires_grad = False\n",
    "\n",
    "    # Convert trainable parameters to float32\n",
    "    for param in model.parameters():\n",
    "        if param.requires_grad:\n",
    "            param.data = param.data.to(torch.float32)\n",
    "\n",
    "    model.print_trainable_parameters()\n",
    "\n",
    "\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BAIzjWgZw5E0"
   },
   "source": [
    "## 5. 토크나이징 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 37,
     "status": "ok",
     "timestamp": 1743855774424,
     "user": {
      "displayName": "정장영 (JEO)",
      "userId": "12533087895771299388"
     },
     "user_tz": -540
    },
    "id": "CWqm2SSiw5E0"
   },
   "outputs": [],
   "source": [
    "def tokenize_function(examples, tokenizer):\n",
    "    result = tokenizer(\n",
    "        examples[\"text\"],\n",
    "        truncation=True,\n",
    "        max_length=MAX_LENGTH,\n",
    "        padding=\"max_length\",\n",
    "        return_tensors=None\n",
    "    )\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2135,
     "status": "ok",
     "timestamp": 1743855776560,
     "user": {
      "displayName": "정장영 (JEO)",
      "userId": "12533087895771299388"
     },
     "user_tz": -540
    },
    "id": "FlmMvY83scQC",
    "outputId": "f6f7723d-070b-419b-b02f-9b39a31fe380"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openpyxl in /usr/local/lib/python3.11/dist-packages (3.1.5)\n",
      "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.11/dist-packages (from openpyxl) (2.0.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install openpyxl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZqxInSW7w5E0"
   },
   "source": [
    "## 6. 데이터 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 52903,
     "status": "ok",
     "timestamp": 1743855829463,
     "user": {
      "displayName": "정장영 (JEO)",
      "userId": "12533087895771299388"
     },
     "user_tz": -540
    },
    "id": "NnNLqlpvw5E0",
    "outputId": "67bbdb18-d262-406c-db96-a76f34d2d5bf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for files in /content/drive/MyDrive/ToyProject/for_Colab/LiarHeart_dataset\n",
      "Found Excel files: ['/content/drive/MyDrive/ToyProject/for_Colab/LiarHeart_dataset/페르소나 데이터_신유영.xlsx', '/content/drive/MyDrive/ToyProject/for_Colab/LiarHeart_dataset/페르소나 데이터_이도윤.xlsx', '/content/drive/MyDrive/ToyProject/for_Colab/LiarHeart_dataset/페르소나 데이터_박강헌.xlsx']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rLoading Excel files:   0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: /content/drive/MyDrive/ToyProject/for_Colab/LiarHeart_dataset/페르소나 데이터_신유영.xlsx\n",
      "  Reading sheet: 알리바이_대화\n",
      "  Columns in sheet 알리바이_대화: ['대분류', '소분류', 'Unnamed: 2', '사람 대사', '챗봇 대사', '감정']\n",
      "  Reading sheet: 인터뷰_대화\n",
      "  Columns in sheet 인터뷰_대화: ['대분류', '소분류', 'Unnamed: 2', '사람 대사', '챗봇 대사', '감정']\n",
      "  Reading sheet: 가쉽_대화\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rLoading Excel files:  33%|███▎      | 1/3 [00:09<00:18,  9.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Columns in sheet 가쉽_대화: ['대분류', '소분류', '사람 대사', '챗봇 대사', '감정']\n",
      "Processing file: /content/drive/MyDrive/ToyProject/for_Colab/LiarHeart_dataset/페르소나 데이터_이도윤.xlsx\n",
      "  Reading sheet: 알리바이_대화\n",
      "  Columns in sheet 알리바이_대화: ['Unnamed: 0', '소분류', 'Unnamed: 2', '사람 대사', '챗봇 대사', '감정']\n",
      "  Reading sheet: 인터뷰_대화\n",
      "  Columns in sheet 인터뷰_대화: ['Unnamed: 0', '소분류', 'Unnamed: 2', '사람 대사', '챗봇 대사', '감정']\n",
      "  Reading sheet: 가쉽_대화\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rLoading Excel files:  67%|██████▋   | 2/3 [00:20<00:10, 10.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Columns in sheet 가쉽_대화: ['대분류', '소분류', '사람 대사', '챗봇 대사', '감정']\n",
      "Processing file: /content/drive/MyDrive/ToyProject/for_Colab/LiarHeart_dataset/페르소나 데이터_박강헌.xlsx\n",
      "  Reading sheet: 알리바이_대화\n",
      "  Columns in sheet 알리바이_대화: ['대분류', '소분류', 'Unnamed: 2', '사람 대사', '챗봇 대사', '감정']\n",
      "  Reading sheet: 인터뷰_대화\n",
      "  Columns in sheet 인터뷰_대화: ['대분류', '소분류', 'Unnamed: 2', '사람 대사', '챗봇 대사', '감정']\n",
      "  Reading sheet: 가쉽_대화\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Excel files: 100%|██████████| 3/3 [00:32<00:00, 10.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Columns in sheet 가쉽_대화: ['대분류', '소분류', '사람 대사', '챗봇 대사', '감정']\n",
      "Total examples loaded: 40760\n",
      "Final columns: ['Q', 'A', 'E', 'N']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 36684\n",
      "Eval dataset size: 4076\n",
      "Sample text from train dataset: Human: 범행 전날 00시에는 누구와 있었나요?\n",
      "Assistant: 저... 자취해서... 혼자 있었어요.......\n",
      "Emotion: 감정없음\n",
      "Name: 신유영\n"
     ]
    }
   ],
   "source": [
    "# 데이터 준비\n",
    "train_dataset, eval_dataset = load_and_prepare_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1743855829472,
     "user": {
      "displayName": "정장영 (JEO)",
      "userId": "12533087895771299388"
     },
     "user_tz": -540
    },
    "id": "n9BCewW_yPg7",
    "outputId": "499c43e2-d2bd-4e00-d653-e3ff0ac59818"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'Human: 범행 전날 00시에는 누구와 있었나요?\\nAssistant: 저... 자취해서... 혼자 있었어요.......\\nEmotion: 감정없음\\nName: 신유영'}\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-pxunhyyw5E0"
   },
   "source": [
    "## 7. 모델 및 토크나이저 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 71591,
     "status": "ok",
     "timestamp": 1743855901063,
     "user": {
      "displayName": "정장영 (JEO)",
      "userId": "12533087895771299388"
     },
     "user_tz": -540
    },
    "id": "UchZLgQjw5E0",
    "outputId": "b51bcbf9-ea7b-40f7-fc52-255fe65adbfc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model and tokenizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n",
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.0.self_attn.o_proj.lora_A.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.0.self_attn.o_proj.lora_B.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.0.mlp.gate_proj.lora_A.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.0.mlp.gate_proj.lora_B.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.0.mlp.up_proj.lora_A.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.0.mlp.up_proj.lora_B.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.0.mlp.down_proj.lora_A.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.0.mlp.down_proj.lora_B.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.1.self_attn.k_proj.lora_A.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.1.self_attn.k_proj.lora_B.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.1.self_attn.o_proj.lora_A.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.1.self_attn.o_proj.lora_B.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.1.mlp.gate_proj.lora_A.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.1.mlp.gate_proj.lora_B.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.1.mlp.up_proj.lora_A.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.1.mlp.up_proj.lora_B.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.1.mlp.down_proj.lora_A.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.1.mlp.down_proj.lora_B.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.2.self_attn.k_proj.lora_A.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.2.self_attn.k_proj.lora_B.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.2.self_attn.o_proj.lora_A.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.2.self_attn.o_proj.lora_B.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.2.mlp.gate_proj.lora_A.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.2.mlp.gate_proj.lora_B.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.2.mlp.up_proj.lora_A.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.2.mlp.up_proj.lora_B.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.2.mlp.down_proj.lora_A.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.2.mlp.down_proj.lora_B.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.3.self_attn.k_proj.lora_A.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.3.self_attn.k_proj.lora_B.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.3.self_attn.o_proj.lora_A.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.3.self_attn.o_proj.lora_B.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.3.mlp.gate_proj.lora_A.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.3.mlp.gate_proj.lora_B.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.3.mlp.up_proj.lora_A.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.3.mlp.up_proj.lora_B.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.3.mlp.down_proj.lora_A.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.3.mlp.down_proj.lora_B.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.4.self_attn.k_proj.lora_A.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.4.self_attn.k_proj.lora_B.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.4.self_attn.o_proj.lora_A.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.4.self_attn.o_proj.lora_B.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.4.mlp.gate_proj.lora_A.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.4.mlp.gate_proj.lora_B.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.4.mlp.up_proj.lora_A.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.4.mlp.up_proj.lora_B.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.4.mlp.down_proj.lora_A.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.4.mlp.down_proj.lora_B.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.5.self_attn.k_proj.lora_A.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.5.self_attn.k_proj.lora_B.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.5.self_attn.o_proj.lora_A.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.5.self_attn.o_proj.lora_B.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.5.mlp.gate_proj.lora_A.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.5.mlp.gate_proj.lora_B.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.5.mlp.up_proj.lora_A.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.5.mlp.up_proj.lora_B.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.5.mlp.down_proj.lora_A.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.5.mlp.down_proj.lora_B.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.6.self_attn.k_proj.lora_A.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.6.self_attn.k_proj.lora_B.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.6.self_attn.o_proj.lora_A.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.6.self_attn.o_proj.lora_B.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.6.mlp.gate_proj.lora_A.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.6.mlp.gate_proj.lora_B.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.6.mlp.up_proj.lora_A.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.6.mlp.up_proj.lora_B.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.6.mlp.down_proj.lora_A.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.6.mlp.down_proj.lora_B.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.7.self_attn.k_proj.lora_A.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.7.self_attn.k_proj.lora_B.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.7.self_attn.o_proj.lora_A.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.7.self_attn.o_proj.lora_B.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.7.mlp.gate_proj.lora_A.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.7.mlp.gate_proj.lora_B.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.7.mlp.up_proj.lora_A.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.7.mlp.up_proj.lora_B.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.7.mlp.down_proj.lora_A.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.7.mlp.down_proj.lora_B.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.8.self_attn.k_proj.lora_A.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.8.self_attn.k_proj.lora_B.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.8.self_attn.o_proj.lora_A.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.8.self_attn.o_proj.lora_B.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.8.mlp.gate_proj.lora_A.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.8.mlp.gate_proj.lora_B.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.8.mlp.up_proj.lora_A.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.8.mlp.up_proj.lora_B.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.8.mlp.down_proj.lora_A.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.8.mlp.down_proj.lora_B.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.9.self_attn.k_proj.lora_A.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.9.self_attn.k_proj.lora_B.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.9.self_attn.o_proj.lora_A.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.9.self_attn.o_proj.lora_B.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.9.mlp.gate_proj.lora_A.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.9.mlp.gate_proj.lora_B.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.9.mlp.up_proj.lora_A.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.9.mlp.up_proj.lora_B.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.9.mlp.down_proj.lora_A.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.9.mlp.down_proj.lora_B.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.10.self_attn.k_proj.lora_A.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.10.self_attn.k_proj.lora_B.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.10.self_attn.o_proj.lora_A.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.10.self_attn.o_proj.lora_B.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.10.mlp.gate_proj.lora_A.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.10.mlp.gate_proj.lora_B.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.10.mlp.up_proj.lora_A.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.10.mlp.up_proj.lora_B.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.10.mlp.down_proj.lora_A.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.10.mlp.down_proj.lora_B.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.11.self_attn.k_proj.lora_A.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.11.self_attn.k_proj.lora_B.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.11.self_attn.o_proj.lora_A.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.11.self_attn.o_proj.lora_B.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.11.mlp.gate_proj.lora_A.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.11.mlp.gate_proj.lora_B.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.11.mlp.up_proj.lora_A.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.11.mlp.up_proj.lora_B.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.11.mlp.down_proj.lora_A.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.11.mlp.down_proj.lora_B.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.12.self_attn.k_proj.lora_A.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.12.self_attn.k_proj.lora_B.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.12.self_attn.o_proj.lora_A.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.12.self_attn.o_proj.lora_B.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.12.mlp.gate_proj.lora_A.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.12.mlp.gate_proj.lora_B.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.12.mlp.up_proj.lora_A.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.12.mlp.up_proj.lora_B.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.12.mlp.down_proj.lora_A.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.12.mlp.down_proj.lora_B.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.13.self_attn.k_proj.lora_A.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.13.self_attn.k_proj.lora_B.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.13.self_attn.o_proj.lora_A.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.13.self_attn.o_proj.lora_B.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.13.mlp.gate_proj.lora_A.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.13.mlp.gate_proj.lora_B.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.13.mlp.up_proj.lora_A.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.13.mlp.up_proj.lora_B.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.13.mlp.down_proj.lora_A.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.13.mlp.down_proj.lora_B.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.14.self_attn.k_proj.lora_A.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.14.self_attn.k_proj.lora_B.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.14.self_attn.o_proj.lora_A.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.14.self_attn.o_proj.lora_B.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.14.mlp.gate_proj.lora_A.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.14.mlp.gate_proj.lora_B.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.14.mlp.up_proj.lora_A.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.14.mlp.up_proj.lora_B.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.14.mlp.down_proj.lora_A.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.14.mlp.down_proj.lora_B.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.15.self_attn.k_proj.lora_A.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.15.self_attn.k_proj.lora_B.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.15.self_attn.o_proj.lora_A.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.15.self_attn.o_proj.lora_B.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.15.mlp.gate_proj.lora_A.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.15.mlp.gate_proj.lora_B.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.15.mlp.up_proj.lora_A.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.15.mlp.up_proj.lora_B.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.15.mlp.down_proj.lora_A.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.15.mlp.down_proj.lora_B.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.16.self_attn.k_proj.lora_A.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.16.self_attn.k_proj.lora_B.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.16.self_attn.o_proj.lora_A.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.16.self_attn.o_proj.lora_B.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.16.mlp.gate_proj.lora_A.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.16.mlp.gate_proj.lora_B.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.16.mlp.up_proj.lora_A.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.16.mlp.up_proj.lora_B.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.16.mlp.down_proj.lora_A.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.16.mlp.down_proj.lora_B.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.17.self_attn.k_proj.lora_A.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.17.self_attn.k_proj.lora_B.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.17.self_attn.o_proj.lora_A.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.17.self_attn.o_proj.lora_B.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.17.mlp.gate_proj.lora_A.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.17.mlp.gate_proj.lora_B.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.17.mlp.up_proj.lora_A.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.17.mlp.up_proj.lora_B.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.17.mlp.down_proj.lora_A.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.17.mlp.down_proj.lora_B.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.18.self_attn.k_proj.lora_A.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.18.self_attn.k_proj.lora_B.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.18.self_attn.o_proj.lora_A.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.18.self_attn.o_proj.lora_B.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.18.mlp.gate_proj.lora_A.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.18.mlp.gate_proj.lora_B.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.18.mlp.up_proj.lora_A.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.18.mlp.up_proj.lora_B.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.18.mlp.down_proj.lora_A.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.18.mlp.down_proj.lora_B.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.19.self_attn.k_proj.lora_A.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.19.self_attn.k_proj.lora_B.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.19.self_attn.o_proj.lora_A.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.19.self_attn.o_proj.lora_B.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.19.mlp.gate_proj.lora_A.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.19.mlp.gate_proj.lora_B.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.19.mlp.up_proj.lora_A.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.19.mlp.up_proj.lora_B.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.19.mlp.down_proj.lora_A.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.19.mlp.down_proj.lora_B.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.20.self_attn.k_proj.lora_A.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.20.self_attn.k_proj.lora_B.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.20.self_attn.o_proj.lora_A.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.20.self_attn.o_proj.lora_B.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.20.mlp.gate_proj.lora_A.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.20.mlp.gate_proj.lora_B.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.20.mlp.up_proj.lora_A.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.20.mlp.up_proj.lora_B.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.20.mlp.down_proj.lora_A.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.20.mlp.down_proj.lora_B.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.21.self_attn.k_proj.lora_A.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.21.self_attn.k_proj.lora_B.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.21.self_attn.o_proj.lora_A.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.21.self_attn.o_proj.lora_B.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.21.mlp.gate_proj.lora_A.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.21.mlp.gate_proj.lora_B.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.21.mlp.up_proj.lora_A.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.21.mlp.up_proj.lora_B.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.21.mlp.down_proj.lora_A.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.21.mlp.down_proj.lora_B.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.22.self_attn.k_proj.lora_A.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.22.self_attn.k_proj.lora_B.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.22.self_attn.o_proj.lora_A.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.22.self_attn.o_proj.lora_B.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.22.mlp.gate_proj.lora_A.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.22.mlp.gate_proj.lora_B.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.22.mlp.up_proj.lora_A.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.22.mlp.up_proj.lora_B.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.22.mlp.down_proj.lora_A.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.22.mlp.down_proj.lora_B.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.23.self_attn.k_proj.lora_A.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.23.self_attn.k_proj.lora_B.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.23.self_attn.v_proj.lora_B.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.23.self_attn.o_proj.lora_A.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.23.self_attn.o_proj.lora_B.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.23.mlp.gate_proj.lora_A.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.23.mlp.gate_proj.lora_B.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.23.mlp.up_proj.lora_A.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.23.mlp.up_proj.lora_B.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.23.mlp.down_proj.lora_A.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.23.mlp.down_proj.lora_B.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.24.self_attn.q_proj.lora_A.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.24.self_attn.q_proj.lora_B.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.24.self_attn.k_proj.lora_A.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.24.self_attn.k_proj.lora_B.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.24.self_attn.v_proj.lora_A.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.24.self_attn.v_proj.lora_B.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.24.self_attn.o_proj.lora_A.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.24.self_attn.o_proj.lora_B.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.24.mlp.gate_proj.lora_A.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.24.mlp.gate_proj.lora_B.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.24.mlp.up_proj.lora_A.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.24.mlp.up_proj.lora_B.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.24.mlp.down_proj.lora_A.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.24.mlp.down_proj.lora_B.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.25.self_attn.q_proj.lora_A.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.25.self_attn.q_proj.lora_B.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.25.self_attn.k_proj.lora_A.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.25.self_attn.k_proj.lora_B.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.25.self_attn.v_proj.lora_A.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.25.self_attn.v_proj.lora_B.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.25.self_attn.o_proj.lora_A.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.25.self_attn.o_proj.lora_B.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.25.mlp.gate_proj.lora_A.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.25.mlp.gate_proj.lora_B.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.25.mlp.up_proj.lora_A.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.25.mlp.up_proj.lora_B.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.25.mlp.down_proj.lora_A.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.25.mlp.down_proj.lora_B.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.26.self_attn.q_proj.lora_A.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.26.self_attn.q_proj.lora_B.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.26.self_attn.k_proj.lora_A.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.26.self_attn.k_proj.lora_B.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.26.self_attn.v_proj.lora_A.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.26.self_attn.v_proj.lora_B.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.26.self_attn.o_proj.lora_A.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.26.self_attn.o_proj.lora_B.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.26.mlp.gate_proj.lora_A.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.26.mlp.gate_proj.lora_B.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.26.mlp.up_proj.lora_A.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.26.mlp.up_proj.lora_B.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.26.mlp.down_proj.lora_A.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.26.mlp.down_proj.lora_B.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.27.self_attn.q_proj.lora_A.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.27.self_attn.q_proj.lora_B.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.27.self_attn.k_proj.lora_A.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.27.self_attn.k_proj.lora_B.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.27.self_attn.v_proj.lora_A.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.27.self_attn.v_proj.lora_B.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.27.self_attn.o_proj.lora_A.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.27.self_attn.o_proj.lora_B.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.27.mlp.gate_proj.lora_A.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.27.mlp.gate_proj.lora_B.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.27.mlp.up_proj.lora_A.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.27.mlp.up_proj.lora_B.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.27.mlp.down_proj.lora_A.default.weight requires_grad: True\n",
      "Parameter base_model.model.model.layers.27.mlp.down_proj.lora_B.default.weight requires_grad: True\n",
      "trainable params: 9,232,384 || all params: 1,786,320,384 || trainable%: 0.5168\n"
     ]
    }
   ],
   "source": [
    "# 모델 및 토크나이저 준비\n",
    "model, tokenizer = prepare_model_and_tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 20,
     "status": "ok",
     "timestamp": 1743855901085,
     "user": {
      "displayName": "정장영 (JEO)",
      "userId": "12533087895771299388"
     },
     "user_tz": -540
    },
    "id": "5x9g4PobzubS",
    "outputId": "0563a36f-9539-4edd-d757-c37793ac8fd4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PeftModelForCausalLM(\n",
      "  (base_model): LoraModel(\n",
      "    (model): Qwen2ForCausalLM(\n",
      "      (model): Qwen2Model(\n",
      "        (embed_tokens): Embedding(151936, 1536)\n",
      "        (layers): ModuleList(\n",
      "          (0-27): 28 x Qwen2DecoderLayer(\n",
      "            (self_attn): Qwen2Attention(\n",
      "              (q_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.15, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=1536, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=1536, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (k_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=1536, out_features=256, bias=True)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.15, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=1536, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=256, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (v_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=1536, out_features=256, bias=True)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.15, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=1536, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=256, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (o_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=1536, out_features=1536, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.15, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=1536, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=1536, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "            )\n",
      "            (mlp): Qwen2MLP(\n",
      "              (gate_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=1536, out_features=8960, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.15, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=1536, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=8960, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (up_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=1536, out_features=8960, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.15, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=1536, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=8960, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (down_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=8960, out_features=1536, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.15, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=8960, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=1536, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (act_fn): SiLU()\n",
      "            )\n",
      "            (input_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
      "            (post_attention_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
      "          )\n",
      "        )\n",
      "        (norm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
      "        (rotary_emb): Qwen2RotaryEmbedding()\n",
      "      )\n",
      "      (lm_head): Linear(in_features=1536, out_features=151936, bias=False)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fh5nnySrzuI8"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1743855901087,
     "user": {
      "displayName": "정장영 (JEO)",
      "userId": "12533087895771299388"
     },
     "user_tz": -540
    },
    "id": "lEhvU_T565ZS"
   },
   "outputs": [],
   "source": [
    "class GPUDataCollator:\n",
    "    def __init__(self, base_collator, device):\n",
    "        self.base_collator = base_collator\n",
    "        self.device = device\n",
    "\n",
    "    def __call__(self, examples):\n",
    "        batch = self.base_collator(examples)\n",
    "        # Move batch to GPU\n",
    "        return {k: v.to(self.device) if isinstance(v, torch.Tensor) else v\n",
    "                for k, v in batch.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gVMBMtoJw5E0"
   },
   "source": [
    "## 8. 데이터 토크나이징"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 99,
     "referenced_widgets": [
      "4784a17f16ee4dee87940837699aa708",
      "fe93a63f5e4047a9b1cd40ce2cc07ce7",
      "9111502f33fd4e0eaa3b99b05c37302c",
      "6358bdd1f78546a680039988fb57e6d5",
      "3431384707354c3885050edb78771c14",
      "e9b782527934465b83d41f7df724910b",
      "2364550c084744d08e2cca0227699e0a",
      "aedb2569b23d44a988ae2e67a50111cb",
      "9cc834a447db4f1eab62cbf9cbac05b6",
      "b13a23ce09ec4bf6ac80d8a2410a0e89",
      "f16121a26b3346e9bed6af932ba9f12c",
      "cb8adfd8301845a9a06764492e7c6c07",
      "556ad9259a0f477787736a0c391ca4d5",
      "458458913c6446d5815f7b77a51ce2ba",
      "1aba694b53d948a08892fd6443322e58",
      "cd80f5b7bcfb4ad69c0480939399c78c",
      "b3c020ed68ca4da295439af1a9eb94db",
      "c11872e8959b467a87e43b36927373ef",
      "8e5993a21697435e91079f29c4026b24",
      "c38433b6a9cb4b2b95fd71938390c23f",
      "c99516de2f02428488c324471e434ef6",
      "ba35a925a35348a1b04bad4d1e8b68b6"
     ]
    },
    "executionInfo": {
     "elapsed": 12575,
     "status": "ok",
     "timestamp": 1743855913721,
     "user": {
      "displayName": "정장영 (JEO)",
      "userId": "12533087895771299388"
     },
     "user_tz": -540
    },
    "id": "zdczziqLw5E0",
    "outputId": "8d97bd2c-6b30-40e6-e39a-2ba7890d15c8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing datasets...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4784a17f16ee4dee87940837699aa708",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/36684 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb8adfd8301845a9a06764492e7c6c07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4076 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# 데이터 토크나이징\n",
    "print(\"Tokenizing datasets...\")\n",
    "tokenized_train = train_dataset.map(\n",
    "    lambda x: tokenize_function(x, tokenizer),\n",
    "    batched=True,\n",
    "    remove_columns=[\"text\"]\n",
    ")\n",
    "\n",
    "tokenized_eval = eval_dataset.map(\n",
    "    lambda x: tokenize_function(x, tokenizer),\n",
    "    batched=True,\n",
    "    remove_columns=[\"text\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1743855913733,
     "user": {
      "displayName": "정장영 (JEO)",
      "userId": "12533087895771299388"
     },
     "user_tz": -540
    },
    "id": "g3Z15BMY7vhg"
   },
   "outputs": [],
   "source": [
    "import torch.multiprocessing as mp\n",
    "mp.set_start_method('spawn', force=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FocLrxJ_w5E0"
   },
   "source": [
    "## 9. 학습 설정 및 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "LVwN0Zfbw5E0",
    "outputId": "91dc2ea1-a7b1-462b-a836-2495c49446d7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model is on device: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "You are adding a <class 'transformers.integrations.integration_utils.TensorBoardCallback'> to the callbacks of this Trainer, but there is already one. The currentlist of callbacks is\n",
      ":DefaultFlowCallback\n",
      "TensorBoardCallback\n",
      "WandbCallback\n",
      "CustomCallback\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "\n",
      "Model state before training:\n",
      "Training mode: True\n",
      "Device: cuda:0\n",
      "Trainable parameters:\n",
      "base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.0.self_attn.o_proj.lora_A.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.0.self_attn.o_proj.lora_B.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.0.mlp.gate_proj.lora_A.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.0.mlp.gate_proj.lora_B.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.0.mlp.up_proj.lora_A.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.0.mlp.up_proj.lora_B.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.0.mlp.down_proj.lora_A.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.0.mlp.down_proj.lora_B.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.1.self_attn.k_proj.lora_A.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.1.self_attn.k_proj.lora_B.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.1.self_attn.o_proj.lora_A.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.1.self_attn.o_proj.lora_B.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.1.mlp.gate_proj.lora_A.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.1.mlp.gate_proj.lora_B.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.1.mlp.up_proj.lora_A.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.1.mlp.up_proj.lora_B.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.1.mlp.down_proj.lora_A.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.1.mlp.down_proj.lora_B.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.2.self_attn.k_proj.lora_A.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.2.self_attn.k_proj.lora_B.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.2.self_attn.o_proj.lora_A.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.2.self_attn.o_proj.lora_B.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.2.mlp.gate_proj.lora_A.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.2.mlp.gate_proj.lora_B.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.2.mlp.up_proj.lora_A.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.2.mlp.up_proj.lora_B.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.2.mlp.down_proj.lora_A.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.2.mlp.down_proj.lora_B.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.3.self_attn.k_proj.lora_A.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.3.self_attn.k_proj.lora_B.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.3.self_attn.o_proj.lora_A.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.3.self_attn.o_proj.lora_B.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.3.mlp.gate_proj.lora_A.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.3.mlp.gate_proj.lora_B.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.3.mlp.up_proj.lora_A.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.3.mlp.up_proj.lora_B.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.3.mlp.down_proj.lora_A.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.3.mlp.down_proj.lora_B.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.4.self_attn.k_proj.lora_A.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.4.self_attn.k_proj.lora_B.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.4.self_attn.o_proj.lora_A.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.4.self_attn.o_proj.lora_B.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.4.mlp.gate_proj.lora_A.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.4.mlp.gate_proj.lora_B.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.4.mlp.up_proj.lora_A.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.4.mlp.up_proj.lora_B.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.4.mlp.down_proj.lora_A.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.4.mlp.down_proj.lora_B.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.5.self_attn.k_proj.lora_A.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.5.self_attn.k_proj.lora_B.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.5.self_attn.o_proj.lora_A.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.5.self_attn.o_proj.lora_B.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.5.mlp.gate_proj.lora_A.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.5.mlp.gate_proj.lora_B.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.5.mlp.up_proj.lora_A.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.5.mlp.up_proj.lora_B.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.5.mlp.down_proj.lora_A.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.5.mlp.down_proj.lora_B.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.6.self_attn.k_proj.lora_A.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.6.self_attn.k_proj.lora_B.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.6.self_attn.o_proj.lora_A.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.6.self_attn.o_proj.lora_B.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.6.mlp.gate_proj.lora_A.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.6.mlp.gate_proj.lora_B.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.6.mlp.up_proj.lora_A.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.6.mlp.up_proj.lora_B.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.6.mlp.down_proj.lora_A.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.6.mlp.down_proj.lora_B.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.7.self_attn.k_proj.lora_A.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.7.self_attn.k_proj.lora_B.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.7.self_attn.o_proj.lora_A.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.7.self_attn.o_proj.lora_B.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.7.mlp.gate_proj.lora_A.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.7.mlp.gate_proj.lora_B.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.7.mlp.up_proj.lora_A.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.7.mlp.up_proj.lora_B.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.7.mlp.down_proj.lora_A.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.7.mlp.down_proj.lora_B.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.8.self_attn.k_proj.lora_A.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.8.self_attn.k_proj.lora_B.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.8.self_attn.o_proj.lora_A.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.8.self_attn.o_proj.lora_B.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.8.mlp.gate_proj.lora_A.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.8.mlp.gate_proj.lora_B.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.8.mlp.up_proj.lora_A.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.8.mlp.up_proj.lora_B.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.8.mlp.down_proj.lora_A.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.8.mlp.down_proj.lora_B.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.9.self_attn.k_proj.lora_A.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.9.self_attn.k_proj.lora_B.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.9.self_attn.o_proj.lora_A.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.9.self_attn.o_proj.lora_B.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.9.mlp.gate_proj.lora_A.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.9.mlp.gate_proj.lora_B.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.9.mlp.up_proj.lora_A.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.9.mlp.up_proj.lora_B.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.9.mlp.down_proj.lora_A.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.9.mlp.down_proj.lora_B.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.10.self_attn.k_proj.lora_A.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.10.self_attn.k_proj.lora_B.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.10.self_attn.o_proj.lora_A.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.10.self_attn.o_proj.lora_B.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.10.mlp.gate_proj.lora_A.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.10.mlp.gate_proj.lora_B.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.10.mlp.up_proj.lora_A.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.10.mlp.up_proj.lora_B.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.10.mlp.down_proj.lora_A.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.10.mlp.down_proj.lora_B.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.11.self_attn.k_proj.lora_A.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.11.self_attn.k_proj.lora_B.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.11.self_attn.o_proj.lora_A.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.11.self_attn.o_proj.lora_B.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.11.mlp.gate_proj.lora_A.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.11.mlp.gate_proj.lora_B.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.11.mlp.up_proj.lora_A.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.11.mlp.up_proj.lora_B.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.11.mlp.down_proj.lora_A.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.11.mlp.down_proj.lora_B.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.12.self_attn.k_proj.lora_A.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.12.self_attn.k_proj.lora_B.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.12.self_attn.o_proj.lora_A.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.12.self_attn.o_proj.lora_B.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.12.mlp.gate_proj.lora_A.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.12.mlp.gate_proj.lora_B.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.12.mlp.up_proj.lora_A.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.12.mlp.up_proj.lora_B.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.12.mlp.down_proj.lora_A.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.12.mlp.down_proj.lora_B.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.13.self_attn.k_proj.lora_A.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.13.self_attn.k_proj.lora_B.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.13.self_attn.o_proj.lora_A.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.13.self_attn.o_proj.lora_B.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.13.mlp.gate_proj.lora_A.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.13.mlp.gate_proj.lora_B.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.13.mlp.up_proj.lora_A.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.13.mlp.up_proj.lora_B.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.13.mlp.down_proj.lora_A.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.13.mlp.down_proj.lora_B.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.14.self_attn.k_proj.lora_A.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.14.self_attn.k_proj.lora_B.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.14.self_attn.o_proj.lora_A.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.14.self_attn.o_proj.lora_B.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.14.mlp.gate_proj.lora_A.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.14.mlp.gate_proj.lora_B.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.14.mlp.up_proj.lora_A.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.14.mlp.up_proj.lora_B.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.14.mlp.down_proj.lora_A.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.14.mlp.down_proj.lora_B.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.15.self_attn.k_proj.lora_A.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.15.self_attn.k_proj.lora_B.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.15.self_attn.o_proj.lora_A.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.15.self_attn.o_proj.lora_B.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.15.mlp.gate_proj.lora_A.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.15.mlp.gate_proj.lora_B.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.15.mlp.up_proj.lora_A.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.15.mlp.up_proj.lora_B.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.15.mlp.down_proj.lora_A.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.15.mlp.down_proj.lora_B.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.16.self_attn.k_proj.lora_A.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.16.self_attn.k_proj.lora_B.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.16.self_attn.o_proj.lora_A.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.16.self_attn.o_proj.lora_B.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.16.mlp.gate_proj.lora_A.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.16.mlp.gate_proj.lora_B.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.16.mlp.up_proj.lora_A.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.16.mlp.up_proj.lora_B.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.16.mlp.down_proj.lora_A.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.16.mlp.down_proj.lora_B.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.17.self_attn.k_proj.lora_A.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.17.self_attn.k_proj.lora_B.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.17.self_attn.o_proj.lora_A.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.17.self_attn.o_proj.lora_B.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.17.mlp.gate_proj.lora_A.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.17.mlp.gate_proj.lora_B.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.17.mlp.up_proj.lora_A.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.17.mlp.up_proj.lora_B.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.17.mlp.down_proj.lora_A.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.17.mlp.down_proj.lora_B.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.18.self_attn.k_proj.lora_A.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.18.self_attn.k_proj.lora_B.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.18.self_attn.o_proj.lora_A.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.18.self_attn.o_proj.lora_B.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.18.mlp.gate_proj.lora_A.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.18.mlp.gate_proj.lora_B.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.18.mlp.up_proj.lora_A.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.18.mlp.up_proj.lora_B.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.18.mlp.down_proj.lora_A.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.18.mlp.down_proj.lora_B.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.19.self_attn.k_proj.lora_A.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.19.self_attn.k_proj.lora_B.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.19.self_attn.o_proj.lora_A.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.19.self_attn.o_proj.lora_B.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.19.mlp.gate_proj.lora_A.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.19.mlp.gate_proj.lora_B.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.19.mlp.up_proj.lora_A.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.19.mlp.up_proj.lora_B.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.19.mlp.down_proj.lora_A.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.19.mlp.down_proj.lora_B.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.20.self_attn.k_proj.lora_A.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.20.self_attn.k_proj.lora_B.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.20.self_attn.o_proj.lora_A.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.20.self_attn.o_proj.lora_B.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.20.mlp.gate_proj.lora_A.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.20.mlp.gate_proj.lora_B.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.20.mlp.up_proj.lora_A.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.20.mlp.up_proj.lora_B.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.20.mlp.down_proj.lora_A.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.20.mlp.down_proj.lora_B.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.21.self_attn.k_proj.lora_A.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.21.self_attn.k_proj.lora_B.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.21.self_attn.o_proj.lora_A.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.21.self_attn.o_proj.lora_B.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.21.mlp.gate_proj.lora_A.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.21.mlp.gate_proj.lora_B.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.21.mlp.up_proj.lora_A.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.21.mlp.up_proj.lora_B.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.21.mlp.down_proj.lora_A.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.21.mlp.down_proj.lora_B.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.22.self_attn.k_proj.lora_A.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.22.self_attn.k_proj.lora_B.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.22.self_attn.o_proj.lora_A.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.22.self_attn.o_proj.lora_B.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.22.mlp.gate_proj.lora_A.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.22.mlp.gate_proj.lora_B.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.22.mlp.up_proj.lora_A.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.22.mlp.up_proj.lora_B.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.22.mlp.down_proj.lora_A.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.22.mlp.down_proj.lora_B.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.23.self_attn.k_proj.lora_A.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.23.self_attn.k_proj.lora_B.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.23.self_attn.v_proj.lora_B.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.23.self_attn.o_proj.lora_A.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.23.self_attn.o_proj.lora_B.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.23.mlp.gate_proj.lora_A.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.23.mlp.gate_proj.lora_B.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.23.mlp.up_proj.lora_A.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.23.mlp.up_proj.lora_B.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.23.mlp.down_proj.lora_A.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.23.mlp.down_proj.lora_B.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.24.self_attn.q_proj.lora_A.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.24.self_attn.q_proj.lora_B.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.24.self_attn.k_proj.lora_A.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.24.self_attn.k_proj.lora_B.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.24.self_attn.v_proj.lora_A.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.24.self_attn.v_proj.lora_B.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.24.self_attn.o_proj.lora_A.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.24.self_attn.o_proj.lora_B.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.24.mlp.gate_proj.lora_A.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.24.mlp.gate_proj.lora_B.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.24.mlp.up_proj.lora_A.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.24.mlp.up_proj.lora_B.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.24.mlp.down_proj.lora_A.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.24.mlp.down_proj.lora_B.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.25.self_attn.q_proj.lora_A.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.25.self_attn.q_proj.lora_B.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.25.self_attn.k_proj.lora_A.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.25.self_attn.k_proj.lora_B.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.25.self_attn.v_proj.lora_A.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.25.self_attn.v_proj.lora_B.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.25.self_attn.o_proj.lora_A.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.25.self_attn.o_proj.lora_B.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.25.mlp.gate_proj.lora_A.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.25.mlp.gate_proj.lora_B.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.25.mlp.up_proj.lora_A.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.25.mlp.up_proj.lora_B.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.25.mlp.down_proj.lora_A.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.25.mlp.down_proj.lora_B.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.26.self_attn.q_proj.lora_A.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.26.self_attn.q_proj.lora_B.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.26.self_attn.k_proj.lora_A.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.26.self_attn.k_proj.lora_B.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.26.self_attn.v_proj.lora_A.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.26.self_attn.v_proj.lora_B.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.26.self_attn.o_proj.lora_A.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.26.self_attn.o_proj.lora_B.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.26.mlp.gate_proj.lora_A.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.26.mlp.gate_proj.lora_B.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.26.mlp.up_proj.lora_A.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.26.mlp.up_proj.lora_B.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.26.mlp.down_proj.lora_A.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.26.mlp.down_proj.lora_B.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.27.self_attn.q_proj.lora_A.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.27.self_attn.q_proj.lora_B.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.27.self_attn.k_proj.lora_A.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.27.self_attn.k_proj.lora_B.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.27.self_attn.v_proj.lora_A.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.27.self_attn.v_proj.lora_B.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.27.self_attn.o_proj.lora_A.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.27.self_attn.o_proj.lora_B.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.27.mlp.gate_proj.lora_A.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.27.mlp.gate_proj.lora_B.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.27.mlp.up_proj.lora_A.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.27.mlp.up_proj.lora_B.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.27.mlp.down_proj.lora_A.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n",
      "base_model.model.model.layers.27.mlp.down_proj.lora_B.default.weight: requires_grad=True, dtype=torch.float32, device=cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "data": {
      "application/javascript": "\n        window._wandbApiKey = new Promise((resolve, reject) => {\n            function loadScript(url) {\n            return new Promise(function(resolve, reject) {\n                let newScript = document.createElement(\"script\");\n                newScript.onerror = reject;\n                newScript.onload = resolve;\n                document.body.appendChild(newScript);\n                newScript.src = url;\n            });\n            }\n            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n            const iframe = document.createElement('iframe')\n            iframe.style.cssText = \"width:0;height:0;border:none\"\n            document.body.appendChild(iframe)\n            const handshake = new Postmate({\n                container: iframe,\n                url: 'https://wandb.ai/authorize'\n            });\n            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n            handshake.then(function(child) {\n                child.on('authorize', data => {\n                    clearTimeout(timeout)\n                    resolve(data)\n                });\n            });\n            })\n        });\n    ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "wandb: Paste an API key from your profile and hit enter:"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ··········\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33makillness38\u001b[0m (\u001b[33makillness38-hongik-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/content/wandb/run-20250405_122734-vtk6yyp6</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/akillness38-hongik-university/huggingface/runs/vtk6yyp6' target=\"_blank\">/content/drive/MyDrive/ToyProject/for_Colab/LiarHeart_dataset/deepseek-r1-finetuned</a></strong> to <a href='https://wandb.ai/akillness38-hongik-university/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/akillness38-hongik-university/huggingface' target=\"_blank\">https://wandb.ai/akillness38-hongik-university/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/akillness38-hongik-university/huggingface/runs/vtk6yyp6' target=\"_blank\">https://wandb.ai/akillness38-hongik-university/huggingface/runs/vtk6yyp6</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get model's device\n",
    "device = next(model.parameters()).device\n",
    "print(f\"\\nModel is on device: {device}\")\n",
    "\n",
    "base_data_collator = DataCollatorForLanguageModeling(\n",
    "        tokenizer=tokenizer,\n",
    "        mlm=False,\n",
    "        pad_to_multiple_of=8\n",
    "    )\n",
    "\n",
    "# Wrap the data collator with GPU support\n",
    "data_collator = GPUDataCollator(base_data_collator, device)\n",
    "\n",
    "os.makedirs(TB_LOG_DIR, exist_ok=True)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    # 기본 설정\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    overwrite_output_dir=True,\n",
    "\n",
    "    # 학습 기본 파라미터\n",
    "    num_train_epochs=3,                    # 증가: 더 나은 수렴을 위해\n",
    "    per_device_train_batch_size=2,         # 감소: VRAM 관리를 위해\n",
    "    per_device_eval_batch_size=4,          # 감소: VRAM 관리를 위해\n",
    "    gradient_accumulation_steps=16,        # 증가: 유효 배치 크기 유지 (2*16=32)\n",
    "\n",
    "    # 옵티마이저 설정\n",
    "    learning_rate=2e-4,                    # 감소: 더 안정적인 학습을 위해\n",
    "    weight_decay=0.01,                     # 유지\n",
    "    warmup_ratio=0.03,                     # 감소: 빠른 초기 학습을 위해\n",
    "\n",
    "    # 저장 및 평가 전략\n",
    "    logging_steps=5,                       # 감소: 더 자주 로깅\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=200,                        # 감소: 더 자주 저장\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=200,                        # 감소: 더 자주 평가\n",
    "    save_total_limit=3,                    # 추가: 디스크 공간 관리\n",
    "\n",
    "    # 메모리 최적화\n",
    "    bf16=True,                             # 유지: L4 GPU에 최적화\n",
    "    gradient_checkpointing=True,           # 유지: 메모리 효율성\n",
    "    dataloader_num_workers=2,              # 감소: 시스템 부하 감소\n",
    "\n",
    "    # 성능 최적화\n",
    "    remove_unused_columns=True,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"loss\",\n",
    "    greater_is_better=False,\n",
    "\n",
    "    # 배치 최적화\n",
    "    group_by_length=True,                  # 유지: 시퀀스 길이별 그룹핑\n",
    "    length_column_name=\"length\",           # 추가: 길이 기반 배치 구성\n",
    "\n",
    "    # 추가 최적화\n",
    "    prediction_loss_only=True,\n",
    "    label_names=[\"labels\"],\n",
    "    ddp_find_unused_parameters=False,      # 추가: DDP 최적화\n",
    "    torch_compile=True,                    # 추가: PyTorch 2.0 컴파일러 활성화\n",
    "\n",
    "    # 메모리 관리\n",
    "    max_grad_norm=1.0,                     # 추가: 그래디언트 클리핑\n",
    "    gradient_checkpointing_kwargs={\n",
    "        \"use_reentrant\": False            # 추가: 메모리 누수 방지\n",
    "    }\n",
    ")\n",
    "\n",
    "# Trainer 초기화 및 학습\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_eval,\n",
    "    data_collator=data_collator,\n",
    "    callbacks=[CustomCallback(), TensorBoardCallback()],\n",
    ")\n",
    "\n",
    "# Move model to device before training\n",
    "model.to(device)\n",
    "\n",
    "# 학습 시작\n",
    "print(\"Starting training...\")\n",
    "try:\n",
    "    # Verify model state before training\n",
    "    print(\"\\nModel state before training:\")\n",
    "    print(f\"Training mode: {model.training}\")\n",
    "    print(f\"Device: {next(model.parameters()).device}\")\n",
    "    print(\"Trainable parameters:\")\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad:\n",
    "            print(f\"{name}: requires_grad={param.requires_grad}, dtype={param.dtype}, device={param.device}\")\n",
    "\n",
    "    # Start training\n",
    "    trainer.train()\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\nError during training: {str(e)}\")\n",
    "    print(\"\\nDebug information:\")\n",
    "    print(f\"Model state: {model.training}\")\n",
    "    print(f\"Device: {next(model.parameters()).device}\")\n",
    "    print(\"Sample data batch:\")\n",
    "    sample_batch = data_collator([tokenized_train[0]])\n",
    "    for k, v in sample_batch.items():\n",
    "        print(f\"{k}: shape {v.shape}, dtype {v.dtype}, device {v.device}\")\n",
    "        # Move tensor to CPU for inspection\n",
    "        v = v.cpu()\n",
    "        print(f\"First few values: {v.flatten()[:5]}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bj3zOqalw5E1"
   },
   "source": [
    "## 10. 모델 저장 및 최종 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K2YdzSd4w5E1"
   },
   "outputs": [],
   "source": [
    "# 모델 저장\n",
    "print(\"Saving model...\")\n",
    "model.save_pretrained(OUTPUT_DIR)\n",
    "tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "\n",
    "# 최종 평가\n",
    "print(\"Final evaluation...\")\n",
    "final_metrics = trainer.evaluate()\n",
    "print(\"\\n===== Final Evaluation Results =====\")\n",
    "for key, value in final_metrics.items():\n",
    "    print(f\"{key}: {value:.4f}\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(f\"Training complete. Model saved to {OUTPUT_DIR}\")\n",
    "print(f\"TensorBoard logs saved to {TB_LOG_DIR}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "L4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "1aba694b53d948a08892fd6443322e58": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c99516de2f02428488c324471e434ef6",
      "placeholder": "​",
      "style": "IPY_MODEL_ba35a925a35348a1b04bad4d1e8b68b6",
      "value": " 4076/4076 [00:01&lt;00:00, 3395.11 examples/s]"
     }
    },
    "2364550c084744d08e2cca0227699e0a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3431384707354c3885050edb78771c14": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "458458913c6446d5815f7b77a51ce2ba": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8e5993a21697435e91079f29c4026b24",
      "max": 4076,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_c38433b6a9cb4b2b95fd71938390c23f",
      "value": 4076
     }
    },
    "4784a17f16ee4dee87940837699aa708": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_fe93a63f5e4047a9b1cd40ce2cc07ce7",
       "IPY_MODEL_9111502f33fd4e0eaa3b99b05c37302c",
       "IPY_MODEL_6358bdd1f78546a680039988fb57e6d5"
      ],
      "layout": "IPY_MODEL_3431384707354c3885050edb78771c14"
     }
    },
    "556ad9259a0f477787736a0c391ca4d5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b3c020ed68ca4da295439af1a9eb94db",
      "placeholder": "​",
      "style": "IPY_MODEL_c11872e8959b467a87e43b36927373ef",
      "value": "Map: 100%"
     }
    },
    "6358bdd1f78546a680039988fb57e6d5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b13a23ce09ec4bf6ac80d8a2410a0e89",
      "placeholder": "​",
      "style": "IPY_MODEL_f16121a26b3346e9bed6af932ba9f12c",
      "value": " 36684/36684 [00:11&lt;00:00, 3311.81 examples/s]"
     }
    },
    "8e5993a21697435e91079f29c4026b24": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9111502f33fd4e0eaa3b99b05c37302c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_aedb2569b23d44a988ae2e67a50111cb",
      "max": 36684,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_9cc834a447db4f1eab62cbf9cbac05b6",
      "value": 36684
     }
    },
    "9cc834a447db4f1eab62cbf9cbac05b6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "aedb2569b23d44a988ae2e67a50111cb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b13a23ce09ec4bf6ac80d8a2410a0e89": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b3c020ed68ca4da295439af1a9eb94db": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ba35a925a35348a1b04bad4d1e8b68b6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c11872e8959b467a87e43b36927373ef": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c38433b6a9cb4b2b95fd71938390c23f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "c99516de2f02428488c324471e434ef6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cb8adfd8301845a9a06764492e7c6c07": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_556ad9259a0f477787736a0c391ca4d5",
       "IPY_MODEL_458458913c6446d5815f7b77a51ce2ba",
       "IPY_MODEL_1aba694b53d948a08892fd6443322e58"
      ],
      "layout": "IPY_MODEL_cd80f5b7bcfb4ad69c0480939399c78c"
     }
    },
    "cd80f5b7bcfb4ad69c0480939399c78c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e9b782527934465b83d41f7df724910b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f16121a26b3346e9bed6af932ba9f12c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "fe93a63f5e4047a9b1cd40ce2cc07ce7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e9b782527934465b83d41f7df724910b",
      "placeholder": "​",
      "style": "IPY_MODEL_2364550c084744d08e2cca0227699e0a",
      "value": "Map: 100%"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
